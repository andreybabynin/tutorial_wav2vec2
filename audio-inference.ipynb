{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-19T16:42:18.636745Z","iopub.execute_input":"2022-12-19T16:42:18.637726Z","iopub.status.idle":"2022-12-19T16:42:22.208416Z","shell.execute_reply.started":"2022-12-19T16:42:18.637635Z","shell.execute_reply":"2022-12-19T16:42:22.206942Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda')\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:42:22.210295Z","iopub.execute_input":"2022-12-19T16:42:22.210855Z","iopub.status.idle":"2022-12-19T16:42:22.229406Z","shell.execute_reply.started":"2022-12-19T16:42:22.210819Z","shell.execute_reply":"2022-12-19T16:42:22.228314Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"vocab_dict = {'m': 0,\n 'ё': 1,\n 'e': 2,\n 'i': 3,\n 'ж': 4,\n '‑': 5,\n 's': 6,\n 'я': 7,\n 'р': 8,\n 'м': 9,\n 'н': 10,\n '«': 11,\n 'й': 12,\n 'g': 13,\n 'т': 14,\n '–': 15,\n 'k': 16,\n 'z': 17,\n '—': 18,\n 'з': 19,\n \"'\": 20,\n 'a': 21,\n 'д': 22,\n 'л': 23,\n '»': 24,\n 'ч': 25,\n 'с': 26,\n 'б': 27,\n 'h': 28,\n 'c': 29,\n '(': 30,\n 'и': 31,\n 'l': 32,\n 'щ': 33,\n 'ф': 34,\n 'o': 35,\n 'ш': 36,\n 'у': 37,\n 'х': 38,\n 'г': 39,\n 'ц': 40,\n '…': 41,\n 'ы': 42,\n 'b': 43,\n 'x': 44,\n 'о': 45,\n 'э': 46,\n 'ъ': 47,\n 'p': 48,\n 'а': 49,\n 'п': 50,\n 'ю': 51,\n '−': 52,\n 'е': 53,\n 'в': 54,\n 'ь': 55,\n ' ': 56,\n 'r': 57,\n 't': 58,\n 'к': 59,\n ')': 60,\n 'f': 61,\n 'n': 62}","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:42:22.232873Z","iopub.execute_input":"2022-12-19T16:42:22.235891Z","iopub.status.idle":"2022-12-19T16:42:22.248885Z","shell.execute_reply.started":"2022-12-19T16:42:22.235854Z","shell.execute_reply":"2022-12-19T16:42:22.247715Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"vocab_dict[\"|\"] = vocab_dict[\" \"]\ndel vocab_dict[\" \"]","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:42:22.255007Z","iopub.execute_input":"2022-12-19T16:42:22.257201Z","iopub.status.idle":"2022-12-19T16:42:22.263589Z","shell.execute_reply.started":"2022-12-19T16:42:22.257163Z","shell.execute_reply":"2022-12-19T16:42:22.262539Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"vocab_dict[\"[UNK]\"] = len(vocab_dict)\nvocab_dict[\"[PAD]\"] = len(vocab_dict)\nlen(vocab_dict)","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:42:22.268845Z","iopub.execute_input":"2022-12-19T16:42:22.271513Z","iopub.status.idle":"2022-12-19T16:42:22.282824Z","shell.execute_reply.started":"2022-12-19T16:42:22.271476Z","shell.execute_reply":"2022-12-19T16:42:22.281470Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"65"},"metadata":{}}]},{"cell_type":"code","source":"import json\nwith open('vocab.json', 'w') as vocab_file:\n    json.dump(vocab_dict, vocab_file)","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:42:22.286221Z","iopub.execute_input":"2022-12-19T16:42:22.288712Z","iopub.status.idle":"2022-12-19T16:42:22.293576Z","shell.execute_reply.started":"2022-12-19T16:42:22.288677Z","shell.execute_reply":"2022-12-19T16:42:22.292583Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"!wget -O model.zip \"https://onedrive.live.com/download?cid=656045B4D0378BFD&resid=656045B4D0378BFD%2116560&authkey=AAURl6JS-nXydU8\"\n!unzip -o model.zip -d model_original","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:42:22.294973Z","iopub.execute_input":"2022-12-19T16:42:22.295641Z","iopub.status.idle":"2022-12-19T16:42:44.910756Z","shell.execute_reply.started":"2022-12-19T16:42:22.295606Z","shell.execute_reply":"2022-12-19T16:42:44.909578Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"--2022-12-19 16:42:23--  https://onedrive.live.com/download?cid=656045B4D0378BFD&resid=656045B4D0378BFD%2116560&authkey=AAURl6JS-nXydU8\nResolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\nConnecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uvwt4w.db.files.1drv.com/y4m01re3dsGsGcxGJY0JT__xoCGcnmPb42iOS-rAmRRBy3lGogYVNog9JxUyFCfX7Q1xiLGmu6H9t0waqMrb5PGNVTTn0CWo6cuSGY8Ofz0VzHriZUUi9Oouh11bEi2oR9Oqy8VKr6yiqyUetyhtHnKSvnAYaG_uYo51X_D70gH36Hue8224xhdlmFTrcWB5CzY5YWDveAmX9i3xeS6WHE2CQ/model.zip?download&psid=1 [following]\n--2022-12-19 16:42:24--  https://uvwt4w.db.files.1drv.com/y4m01re3dsGsGcxGJY0JT__xoCGcnmPb42iOS-rAmRRBy3lGogYVNog9JxUyFCfX7Q1xiLGmu6H9t0waqMrb5PGNVTTn0CWo6cuSGY8Ofz0VzHriZUUi9Oouh11bEi2oR9Oqy8VKr6yiqyUetyhtHnKSvnAYaG_uYo51X_D70gH36Hue8224xhdlmFTrcWB5CzY5YWDveAmX9i3xeS6WHE2CQ/model.zip?download&psid=1\nResolving uvwt4w.db.files.1drv.com (uvwt4w.db.files.1drv.com)... 13.107.42.12\nConnecting to uvwt4w.db.files.1drv.com (uvwt4w.db.files.1drv.com)|13.107.42.12|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 344969172 (329M) [application/zip]\nSaving to: ‘model.zip’\n\nmodel.zip           100%[===================>] 328.99M  23.1MB/s    in 15s     \n\n2022-12-19 16:42:39 (21.9 MB/s) - ‘model.zip’ saved [344969172/344969172]\n\nArchive:  model.zip\n   creating: model_original/model/\n  inflating: model_original/model/config.json  \n  inflating: model_original/model/preprocessor_config.json  \n  inflating: model_original/model/pytorch_model.bin  \n  inflating: model_original/model/rng_state.pth  \n  inflating: model_original/model/scaler.pt  \n  inflating: model_original/model/scheduler.pt  \n  inflating: model_original/model/trainer_state.json  \n  inflating: model_original/model/training_args.bin  \n","output_type":"stream"}]},{"cell_type":"code","source":"!wget -O model1.zip \"https://onedrive.live.com/download?cid=656045B4D0378BFD&resid=656045B4D0378BFD%2116577&authkey=AOCI_APp5FzjxGw\"\n!unzip -o model1.zip -d last_model","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:42:44.912786Z","iopub.execute_input":"2022-12-19T16:42:44.913185Z","iopub.status.idle":"2022-12-19T16:43:06.326054Z","shell.execute_reply.started":"2022-12-19T16:42:44.913129Z","shell.execute_reply":"2022-12-19T16:43:06.324634Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"--2022-12-19 16:42:45--  https://onedrive.live.com/download?cid=656045B4D0378BFD&resid=656045B4D0378BFD%2116577&authkey=AOCI_APp5FzjxGw\nResolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\nConnecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://ulwm4w.db.files.1drv.com/y4maKSytUfJME003aqvXsLYOIuRozQ-muas8i64DU9HmzN_OxLYrZgnpkO9p6VIP2jGg48I8Ie0LysCpdEeH1koYd7Re_OtEp21lkPsH7OMZ-i2swYvAg56hf6YT7FL0Y5a1cn7DOiNTZbSAfQr-WpsfheVZz_6id33jdLC4hiH0Cm68CkYPCiHgWWwL3AoA66AO2rWlx_9vYuV9Z04mZantw/checkpoint-1446.zip?download&psid=1 [following]\n--2022-12-19 16:42:46--  https://ulwm4w.db.files.1drv.com/y4maKSytUfJME003aqvXsLYOIuRozQ-muas8i64DU9HmzN_OxLYrZgnpkO9p6VIP2jGg48I8Ie0LysCpdEeH1koYd7Re_OtEp21lkPsH7OMZ-i2swYvAg56hf6YT7FL0Y5a1cn7DOiNTZbSAfQr-WpsfheVZz_6id33jdLC4hiH0Cm68CkYPCiHgWWwL3AoA66AO2rWlx_9vYuV9Z04mZantw/checkpoint-1446.zip?download&psid=1\nResolving ulwm4w.db.files.1drv.com (ulwm4w.db.files.1drv.com)... 13.107.42.12\nConnecting to ulwm4w.db.files.1drv.com (ulwm4w.db.files.1drv.com)|13.107.42.12|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 345439269 (329M) [application/zip]\nSaving to: ‘model1.zip’\n\nmodel1.zip          100%[===================>] 329.44M  24.2MB/s    in 14s     \n\n2022-12-19 16:43:01 (23.6 MB/s) - ‘model1.zip’ saved [345439269/345439269]\n\nArchive:  model1.zip\n  inflating: last_model/rng_state.pth  \n  inflating: last_model/scaler.pt    \n  inflating: last_model/scheduler.pt  \n  inflating: last_model/trainer_state.json  \n  inflating: last_model/training_args.bin  \n  inflating: last_model/config.json  \n  inflating: last_model/preprocessor_config.json  \n  inflating: last_model/pytorch_model.bin  \n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset, load_metric, Audio\n\n# common_voice_train = load_dataset(\"bond005/sberdevices_golos_100h_farfield\", split=\"train\", streaming=True)\ncommon_voice_test = load_dataset(\"bond005/sberdevices_golos_100h_farfield\", split=\"test\", streaming=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:43:24.966794Z","iopub.execute_input":"2022-12-19T16:43:24.967204Z","iopub.status.idle":"2022-12-19T16:43:26.872235Z","shell.execute_reply.started":"2022-12-19T16:43:24.967166Z","shell.execute_reply":"2022-12-19T16:43:26.871181Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.73k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e9b89c6b17842d0ad48b0e5f66191c6"}},"metadata":{}}]},{"cell_type":"code","source":"# common_voice_train = common_voice_train.cast_column(\"audio\", Audio(sampling_rate=16_000))\ncommon_voice_test = common_voice_test.cast_column(\"audio\", Audio(sampling_rate=16_000))","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:43:39.220437Z","iopub.execute_input":"2022-12-19T16:43:39.221358Z","iopub.status.idle":"2022-12-19T16:43:39.227163Z","shell.execute_reply.started":"2022-12-19T16:43:39.221320Z","shell.execute_reply":"2022-12-19T16:43:39.226001Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from datasets import ClassLabel\nimport random\nimport pandas as pd\nfrom IPython.display import display, HTML","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:43:46.555201Z","iopub.execute_input":"2022-12-19T16:43:46.555680Z","iopub.status.idle":"2022-12-19T16:43:46.561657Z","shell.execute_reply.started":"2022-12-19T16:43:46.555636Z","shell.execute_reply":"2022-12-19T16:43:46.560337Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import re\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�]'\n\ndef remove_special_characters(batch):\n#     batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower() + \" \"\n    try:\n        batch[\"transcription\"] = re.sub(chars_to_ignore_regex, '', batch[\"transcription\"]).lower() + \" \"\n    except:\n        batch[\"transcription\"] = \"й\" #replace empty string\n    return batch","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:43:53.048028Z","iopub.execute_input":"2022-12-19T16:43:53.048401Z","iopub.status.idle":"2022-12-19T16:43:53.055722Z","shell.execute_reply.started":"2022-12-19T16:43:53.048370Z","shell.execute_reply":"2022-12-19T16:43:53.054589Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# common_voice_train = common_voice_train.map(remove_special_characters)\ncommon_voice_test = common_voice_test.map(remove_special_characters)","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:44:01.217893Z","iopub.execute_input":"2022-12-19T16:44:01.218264Z","iopub.status.idle":"2022-12-19T16:44:01.223690Z","shell.execute_reply.started":"2022-12-19T16:44:01.218231Z","shell.execute_reply":"2022-12-19T16:44:01.222596Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from transformers import Wav2Vec2CTCTokenizer\nfrom transformers import Wav2Vec2FeatureExtractor\nfrom transformers import Wav2Vec2Processor\n\ntokenizer = Wav2Vec2CTCTokenizer(\"/kaggle/working/vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n\nfeature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n\nprocessor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:44:13.540037Z","iopub.execute_input":"2022-12-19T16:44:13.540429Z","iopub.status.idle":"2022-12-19T16:44:14.345003Z","shell.execute_reply.started":"2022-12-19T16:44:13.540396Z","shell.execute_reply":"2022-12-19T16:44:14.343892Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import IPython.display as ipd\nimport numpy as np\nimport random","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:44:21.311311Z","iopub.execute_input":"2022-12-19T16:44:21.311692Z","iopub.status.idle":"2022-12-19T16:44:21.318768Z","shell.execute_reply.started":"2022-12-19T16:44:21.311660Z","shell.execute_reply":"2022-12-19T16:44:21.317212Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def prepare_dataset(batch):\n    audio = batch[\"audio\"]\n\n    # batched output is \"un-batched\"\n    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n    \n    with processor.as_target_processor():\n#         batch[\"labels\"] = processor(batch[\"sentence\"]).input_ids\n        batch[\"labels\"] = processor(batch[\"transcription\"]).input_ids\n        \n    return batch","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:44:31.319171Z","iopub.execute_input":"2022-12-19T16:44:31.319542Z","iopub.status.idle":"2022-12-19T16:44:31.325268Z","shell.execute_reply.started":"2022-12-19T16:44:31.319496Z","shell.execute_reply":"2022-12-19T16:44:31.324245Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# common_voice_train = common_voice_train.map(prepare_dataset)\ncommon_voice_test = common_voice_test.map(prepare_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:44:38.049447Z","iopub.execute_input":"2022-12-19T16:44:38.049842Z","iopub.status.idle":"2022-12-19T16:44:38.055482Z","shell.execute_reply.started":"2022-12-19T16:44:38.049810Z","shell.execute_reply":"2022-12-19T16:44:38.054538Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchaudio\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Union\n\n@dataclass\nclass DataCollatorCTCWithPadding:\n    \"\"\"\n    Data collator that will dynamically pad the inputs received.\n    Args:\n        processor (:class:`~transformers.Wav2Vec2Processor`)\n            The processor used for proccessing the data.\n        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n            among:\n            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n              sequence if provided).\n            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n              maximum acceptable input length for the model if that argument is not provided.\n            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n              different lengths).\n        max_length (:obj:`int`, `optional`):\n            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n        max_length_labels (:obj:`int`, `optional`):\n            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n        pad_to_multiple_of (:obj:`int`, `optional`):\n            If set will pad the sequence to a multiple of the provided value.\n            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n            7.5 (Volta).\n    \"\"\"\n\n    processor: Wav2Vec2Processor\n    padding: Union[bool, str] = True\n    max_length: Optional[int] = None\n    max_length_labels: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    pad_to_multiple_of_labels: Optional[int] = None\n          \n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # split inputs and labels since they have to be of different lenghts and need\n        # different padding methods\n        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n\n        batch = self.processor.pad(\n            input_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=\"pt\",\n        )\n        \n        with self.processor.as_target_processor():\n            labels_batch = self.processor.pad(\n                label_features,\n                padding=self.padding,\n                max_length=self.max_length_labels,\n                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n                return_tensors=\"pt\",\n            )\n\n        # replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n        batch[\"labels\"] = labels\n\n        return batch","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:44:57.150994Z","iopub.execute_input":"2022-12-19T16:44:57.151420Z","iopub.status.idle":"2022-12-19T16:44:57.455428Z","shell.execute_reply.started":"2022-12-19T16:44:57.151387Z","shell.execute_reply":"2022-12-19T16:44:57.454298Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:45:06.700489Z","iopub.execute_input":"2022-12-19T16:45:06.701269Z","iopub.status.idle":"2022-12-19T16:45:06.706504Z","shell.execute_reply.started":"2022-12-19T16:45:06.701211Z","shell.execute_reply":"2022-12-19T16:45:06.705431Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install jiwer","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:45:11.900405Z","iopub.execute_input":"2022-12-19T16:45:11.901617Z","iopub.status.idle":"2022-12-19T16:45:25.342202Z","shell.execute_reply.started":"2022-12-19T16:45:11.901553Z","shell.execute_reply":"2022-12-19T16:45:25.340873Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"wer_metric = load_metric(\"wer\")","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:45:25.346635Z","iopub.execute_input":"2022-12-19T16:45:25.346957Z","iopub.status.idle":"2022-12-19T16:45:26.075603Z","shell.execute_reply.started":"2022-12-19T16:45:25.346926Z","shell.execute_reply":"2022-12-19T16:45:26.074649Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.90k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07a2f88d5e164eed9e3006ab855f23c0"}},"metadata":{}}]},{"cell_type":"code","source":"def compute_metrics(pred):\n    pred_logits = pred.predictions\n    pred_ids = np.argmax(pred_logits, axis=-1)\n\n    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n\n    pred_str = processor.batch_decode(pred_ids)\n    # we do not want to group tokens when computing the metrics\n    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n\n    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n\n    return {\"wer\": wer}","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:45:29.468416Z","iopub.execute_input":"2022-12-19T16:45:29.469102Z","iopub.status.idle":"2022-12-19T16:45:29.475563Z","shell.execute_reply.started":"2022-12-19T16:45:29.469067Z","shell.execute_reply":"2022-12-19T16:45:29.474464Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from transformers import Wav2Vec2ForCTC\n\n\nmodel_original = Wav2Vec2ForCTC.from_pretrained(\n#     \"facebook/wav2vec2-base-100k-voxpopuli\", \n#     '/kaggle/working/model_folder/model',\n    '/kaggle/working/model_original/model', \n    attention_dropout=0.1,\n    hidden_dropout=0.1,\n    feat_proj_dropout=0.0,\n    mask_time_prob=0.05,\n    layerdrop=0.1,\n    ctc_loss_reduction=\"mean\", \n    pad_token_id=processor.tokenizer.pad_token_id,\n    vocab_size=len(processor.tokenizer)\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:46:26.782268Z","iopub.execute_input":"2022-12-19T16:46:26.782700Z","iopub.status.idle":"2022-12-19T16:46:28.172893Z","shell.execute_reply.started":"2022-12-19T16:46:26.782662Z","shell.execute_reply":"2022-12-19T16:46:28.171733Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"last_model = Wav2Vec2ForCTC.from_pretrained(\n#     \"facebook/wav2vec2-base-100k-voxpopuli\", \n#     '/kaggle/working/model_folder/model',\n    '/kaggle/working/last_model', \n    attention_dropout=0.1,\n    hidden_dropout=0.1,\n    feat_proj_dropout=0.0,\n    mask_time_prob=0.05,\n    layerdrop=0.1,\n    ctc_loss_reduction=\"mean\", \n    pad_token_id=processor.tokenizer.pad_token_id,\n    vocab_size=len(processor.tokenizer)\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:46:57.420928Z","iopub.execute_input":"2022-12-19T16:46:57.421297Z","iopub.status.idle":"2022-12-19T16:46:58.787118Z","shell.execute_reply.started":"2022-12-19T16:46:57.421265Z","shell.execute_reply":"2022-12-19T16:46:58.785934Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n  # output_dir=\"/content/gdrive/MyDrive/wav2vec2-large-xlsr-turkish-demo\",\n  output_dir=\"./wav2vec2-basee-ru-demo\",\n  group_by_length=False,\n  per_device_train_batch_size=25,\n  gradient_accumulation_steps=16,\n  evaluation_strategy=\"steps\",\n  num_train_epochs=300,\n  fp16=True,\n  save_steps=50,\n  eval_steps=50,\n  logging_steps=10,\n  learning_rate=3e-4,\n  warmup_steps=50,\n  save_total_limit=1,\n    max_steps=300,\n    report_to='none'\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:48:30.769963Z","iopub.execute_input":"2022-12-19T16:48:30.770329Z","iopub.status.idle":"2022-12-19T16:48:30.777846Z","shell.execute_reply.started":"2022-12-19T16:48:30.770298Z","shell.execute_reply":"2022-12-19T16:48:30.776594Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"#### Evaluation of the original model","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer_original = Trainer(\n    model=model_original,\n    data_collator=data_collator,\n    args=training_args,\n    compute_metrics=compute_metrics,\n#     train_dataset=common_voice_train.with_format(\"torch\"),\n    eval_dataset=common_voice_test.with_format(\"torch\"),\n#     train_dataset=common_voice_train,\n#     eval_dataset=common_voice_test,\n    tokenizer=processor.feature_extractor,\n#     optimizers=(optimizer, scheduler)\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:49:22.745743Z","iopub.execute_input":"2022-12-19T16:49:22.746125Z","iopub.status.idle":"2022-12-19T16:49:30.971373Z","shell.execute_reply.started":"2022-12-19T16:49:22.746093Z","shell.execute_reply":"2022-12-19T16:49:30.970436Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\nUsing cuda_amp half precision backend\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer_original.evaluate()","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:51:03.995019Z","iopub.execute_input":"2022-12-19T16:51:03.995412Z","iopub.status.idle":"2022-12-19T16:51:48.531554Z","shell.execute_reply.started":"2022-12-19T16:51:03.995377Z","shell.execute_reply":"2022-12-19T16:51:48.530558Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples: Unknown\n  Batch size = 16\nThe following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: audio, transcription. If audio, transcription are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 1.8427196741104126,\n 'eval_wer': 1.0874388643683646,\n 'eval_runtime': 44.5184,\n 'eval_samples_per_second': 43.038,\n 'eval_steps_per_second': 2.696}"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Evaluation of the trained model","metadata":{}},{"cell_type":"code","source":"trainer_last = Trainer(\n    model=last_model,\n    data_collator=data_collator,\n    args=training_args,\n    compute_metrics=compute_metrics,\n#     train_dataset=common_voice_train.with_format(\"torch\"),\n    eval_dataset=common_voice_test.with_format(\"torch\"),\n#     train_dataset=common_voice_train,\n#     eval_dataset=common_voice_test,\n    tokenizer=processor.feature_extractor,\n#     optimizers=(optimizer, scheduler)\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:52:56.909917Z","iopub.execute_input":"2022-12-19T16:52:56.910549Z","iopub.status.idle":"2022-12-19T16:52:57.022584Z","shell.execute_reply.started":"2022-12-19T16:52:56.910498Z","shell.execute_reply":"2022-12-19T16:52:57.021485Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\nUsing cuda_amp half precision backend\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer_last.evaluate()","metadata":{"execution":{"iopub.status.busy":"2022-12-19T16:53:10.593050Z","iopub.execute_input":"2022-12-19T16:53:10.593414Z","iopub.status.idle":"2022-12-19T16:53:35.518166Z","shell.execute_reply.started":"2022-12-19T16:53:10.593382Z","shell.execute_reply":"2022-12-19T16:53:35.516957Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples: Unknown\n  Batch size = 16\nThe following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: audio, transcription. If audio, transcription are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.631321907043457,\n 'eval_wer': 0.6696886556125492,\n 'eval_runtime': 24.9086,\n 'eval_samples_per_second': 76.921,\n 'eval_steps_per_second': 4.818}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}