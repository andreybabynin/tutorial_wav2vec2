{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-12-18T19:59:50.073151Z","iopub.execute_input":"2022-12-18T19:59:50.074076Z","iopub.status.idle":"2022-12-18T19:59:52.226290Z","shell.execute_reply.started":"2022-12-18T19:59:50.073953Z","shell.execute_reply":"2022-12-18T19:59:52.224789Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda')\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-12-18T19:59:52.228526Z","iopub.execute_input":"2022-12-18T19:59:52.229090Z","iopub.status.idle":"2022-12-18T19:59:52.241487Z","shell.execute_reply.started":"2022-12-18T19:59:52.229050Z","shell.execute_reply":"2022-12-18T19:59:52.239692Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"vocab_dict = {'m': 0,\n 'ё': 1,\n 'e': 2,\n 'i': 3,\n 'ж': 4,\n '‑': 5,\n 's': 6,\n 'я': 7,\n 'р': 8,\n 'м': 9,\n 'н': 10,\n '«': 11,\n 'й': 12,\n 'g': 13,\n 'т': 14,\n '–': 15,\n 'k': 16,\n 'z': 17,\n '—': 18,\n 'з': 19,\n \"'\": 20,\n 'a': 21,\n 'д': 22,\n 'л': 23,\n '»': 24,\n 'ч': 25,\n 'с': 26,\n 'б': 27,\n 'h': 28,\n 'c': 29,\n '(': 30,\n 'и': 31,\n 'l': 32,\n 'щ': 33,\n 'ф': 34,\n 'o': 35,\n 'ш': 36,\n 'у': 37,\n 'х': 38,\n 'г': 39,\n 'ц': 40,\n '…': 41,\n 'ы': 42,\n 'b': 43,\n 'x': 44,\n 'о': 45,\n 'э': 46,\n 'ъ': 47,\n 'p': 48,\n 'а': 49,\n 'п': 50,\n 'ю': 51,\n '−': 52,\n 'е': 53,\n 'в': 54,\n 'ь': 55,\n ' ': 56,\n 'r': 57,\n 't': 58,\n 'к': 59,\n ')': 60,\n 'f': 61,\n 'n': 62}","metadata":{"execution":{"iopub.status.busy":"2022-12-18T19:59:52.245024Z","iopub.execute_input":"2022-12-18T19:59:52.246086Z","iopub.status.idle":"2022-12-18T19:59:52.264635Z","shell.execute_reply.started":"2022-12-18T19:59:52.246044Z","shell.execute_reply":"2022-12-18T19:59:52.263634Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"vocab_dict[\"|\"] = vocab_dict[\" \"]\ndel vocab_dict[\" \"]","metadata":{"execution":{"iopub.status.busy":"2022-12-18T19:59:52.269780Z","iopub.execute_input":"2022-12-18T19:59:52.272486Z","iopub.status.idle":"2022-12-18T19:59:52.279811Z","shell.execute_reply.started":"2022-12-18T19:59:52.272447Z","shell.execute_reply":"2022-12-18T19:59:52.278830Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"vocab_dict[\"[UNK]\"] = len(vocab_dict)\nvocab_dict[\"[PAD]\"] = len(vocab_dict)\nlen(vocab_dict)","metadata":{"execution":{"iopub.status.busy":"2022-12-18T19:59:52.281191Z","iopub.execute_input":"2022-12-18T19:59:52.282180Z","iopub.status.idle":"2022-12-18T19:59:52.294418Z","shell.execute_reply.started":"2022-12-18T19:59:52.282145Z","shell.execute_reply":"2022-12-18T19:59:52.293472Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"65"},"metadata":{}}]},{"cell_type":"code","source":"import json\nwith open('vocab.json', 'w') as vocab_file:\n    json.dump(vocab_dict, vocab_file)","metadata":{"execution":{"iopub.status.busy":"2022-12-18T19:59:52.476991Z","iopub.execute_input":"2022-12-18T19:59:52.477554Z","iopub.status.idle":"2022-12-18T19:59:52.483624Z","shell.execute_reply.started":"2022-12-18T19:59:52.477511Z","shell.execute_reply":"2022-12-18T19:59:52.482709Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# !wget -O model.zip \"https://onedrive.live.com/download?cid=656045B4D0378BFD&resid=656045B4D0378BFD%2116560&authkey=AAURl6JS-nXydU8\"\n# !unzip -o model.zip -d model_folder","metadata":{"execution":{"iopub.status.busy":"2022-12-18T19:59:53.653362Z","iopub.execute_input":"2022-12-18T19:59:53.653849Z","iopub.status.idle":"2022-12-18T19:59:53.658574Z","shell.execute_reply.started":"2022-12-18T19:59:53.653809Z","shell.execute_reply":"2022-12-18T19:59:53.657616Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# 200 checkpoint\n# !wget -O model.zip \"https://onedrive.live.com/download?cid=656045B4D0378BFD&resid=656045B4D0378BFD%2116563&authkey=AJwTvcA4r6gLx3s\"\n# !unzip -o model.zip -d model_folder","metadata":{"execution":{"iopub.status.busy":"2022-12-18T19:59:54.026459Z","iopub.execute_input":"2022-12-18T19:59:54.026909Z","iopub.status.idle":"2022-12-18T19:59:54.033122Z","shell.execute_reply.started":"2022-12-18T19:59:54.026869Z","shell.execute_reply":"2022-12-18T19:59:54.032095Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#546 checkpoint\n# !wget -O model.zip \"https://onedrive.live.com/download?cid=656045B4D0378BFD&resid=656045B4D0378BFD%2116574&authkey=ANBDXT4UHx1JBGE\"\n# !unzip -o model.zip -d model_folder","metadata":{"execution":{"iopub.status.busy":"2022-12-18T19:59:54.415246Z","iopub.execute_input":"2022-12-18T19:59:54.415629Z","iopub.status.idle":"2022-12-18T19:59:54.420506Z","shell.execute_reply.started":"2022-12-18T19:59:54.415596Z","shell.execute_reply":"2022-12-18T19:59:54.419261Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#846 checkpoint\n# !wget -O model.zip \"https://onedrive.live.com/download?cid=656045B4D0378BFD&resid=656045B4D0378BFD%2116575&authkey=AMPpp7yNM2alUXs\"\n# !unzip -o model.zip -d model_folder","metadata":{"execution":{"iopub.status.busy":"2022-12-18T19:59:54.931077Z","iopub.execute_input":"2022-12-18T19:59:54.932140Z","iopub.status.idle":"2022-12-18T19:59:54.936710Z","shell.execute_reply.started":"2022-12-18T19:59:54.932101Z","shell.execute_reply":"2022-12-18T19:59:54.935475Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#1146 checkpoint\n!wget -O model.zip \"https://onedrive.live.com/download?cid=656045B4D0378BFD&resid=656045B4D0378BFD%2116576&authkey=ANSmIkXaqLUWX-E\"\n!unzip -o model.zip -d model_folder","metadata":{"execution":{"iopub.status.busy":"2022-12-18T19:59:55.319275Z","iopub.execute_input":"2022-12-18T19:59:55.319941Z","iopub.status.idle":"2022-12-18T20:00:27.169294Z","shell.execute_reply.started":"2022-12-18T19:59:55.319905Z","shell.execute_reply":"2022-12-18T20:00:27.168134Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"--2022-12-18 19:59:56--  https://onedrive.live.com/download?cid=656045B4D0378BFD&resid=656045B4D0378BFD%2116576&authkey=ANSmIkXaqLUWX-E\nResolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\nConnecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://ulwn4w.db.files.1drv.com/y4mEmeTIadF3k3bI1eRH_MvokKhiEkFHwFfI1CX6r2NtLj0EubbrZsp0D9i-XnTRVZKBB_a8p5um6rrahmADKuIMSWsWy2mTozJDMtAitsYJFY-Jfe-419cmMZ7gViaFwTaMxhj-xzLlaDunoJcxXAIuxLa0sRr1mWE_4TszNsPzJj5q3f-iZ8KGCYs4k5gclhm-U-_bXiK-5eSFMWOzHoO-A/checkpoint-1146.zip?download&psid=1 [following]\n--2022-12-18 19:59:57--  https://ulwn4w.db.files.1drv.com/y4mEmeTIadF3k3bI1eRH_MvokKhiEkFHwFfI1CX6r2NtLj0EubbrZsp0D9i-XnTRVZKBB_a8p5um6rrahmADKuIMSWsWy2mTozJDMtAitsYJFY-Jfe-419cmMZ7gViaFwTaMxhj-xzLlaDunoJcxXAIuxLa0sRr1mWE_4TszNsPzJj5q3f-iZ8KGCYs4k5gclhm-U-_bXiK-5eSFMWOzHoO-A/checkpoint-1146.zip?download&psid=1\nResolving ulwn4w.db.files.1drv.com (ulwn4w.db.files.1drv.com)... 13.107.42.12\nConnecting to ulwn4w.db.files.1drv.com (ulwn4w.db.files.1drv.com)|13.107.42.12|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 345442816 (329M) [application/zip]\nSaving to: ‘model.zip’\n\nmodel.zip           100%[===================>] 329.44M  29.7MB/s    in 13s     \n\n2022-12-18 20:00:22 (25.6 MB/s) - ‘model.zip’ saved [345442816/345442816]\n\nArchive:  model.zip\n  inflating: model_folder/trainer_state.json  \n  inflating: model_folder/training_args.bin  \n  inflating: model_folder/config.json  \n  inflating: model_folder/preprocessor_config.json  \n  inflating: model_folder/pytorch_model.bin  \n  inflating: model_folder/rng_state.pth  \n  inflating: model_folder/scaler.pt  \n  inflating: model_folder/scheduler.pt  \n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset, load_metric, Audio\n\ncommon_voice_train = load_dataset(\"bond005/sberdevices_golos_100h_farfield\", split=\"train\", streaming=True)\ncommon_voice_test = load_dataset(\"bond005/sberdevices_golos_100h_farfield\", split=\"test\", streaming=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:27.172453Z","iopub.execute_input":"2022-12-18T20:00:27.172782Z","iopub.status.idle":"2022-12-18T20:00:29.336298Z","shell.execute_reply.started":"2022-12-18T20:00:27.172752Z","shell.execute_reply":"2022-12-18T20:00:29.335381Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.73k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6288ddf708954a9a998b6200f5249f28"}},"metadata":{}}]},{"cell_type":"code","source":"common_voice_train = common_voice_train.cast_column(\"audio\", Audio(sampling_rate=16_000))\ncommon_voice_test = common_voice_test.cast_column(\"audio\", Audio(sampling_rate=16_000))","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:29.337766Z","iopub.execute_input":"2022-12-18T20:00:29.338358Z","iopub.status.idle":"2022-12-18T20:00:29.345741Z","shell.execute_reply.started":"2022-12-18T20:00:29.338320Z","shell.execute_reply":"2022-12-18T20:00:29.344050Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from datasets import ClassLabel\nimport random\nimport pandas as pd\nfrom IPython.display import display, HTML","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:29.348718Z","iopub.execute_input":"2022-12-18T20:00:29.349425Z","iopub.status.idle":"2022-12-18T20:00:29.355834Z","shell.execute_reply.started":"2022-12-18T20:00:29.349374Z","shell.execute_reply":"2022-12-18T20:00:29.354822Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import re\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�]'\n\ndef remove_special_characters(batch):\n#     batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower() + \" \"\n    try:\n        batch[\"transcription\"] = re.sub(chars_to_ignore_regex, '', batch[\"transcription\"]).lower() + \" \"\n    except:\n        batch[\"transcription\"] = \"й\" #replace empty string\n    return batch","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:29.357504Z","iopub.execute_input":"2022-12-18T20:00:29.357911Z","iopub.status.idle":"2022-12-18T20:00:29.367782Z","shell.execute_reply.started":"2022-12-18T20:00:29.357877Z","shell.execute_reply":"2022-12-18T20:00:29.366858Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"common_voice_train = common_voice_train.map(remove_special_characters)\ncommon_voice_test = common_voice_test.map(remove_special_characters)","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:29.368774Z","iopub.execute_input":"2022-12-18T20:00:29.369025Z","iopub.status.idle":"2022-12-18T20:00:29.379127Z","shell.execute_reply.started":"2022-12-18T20:00:29.368991Z","shell.execute_reply":"2022-12-18T20:00:29.378106Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from transformers import Wav2Vec2CTCTokenizer\nfrom transformers import Wav2Vec2FeatureExtractor\nfrom transformers import Wav2Vec2Processor\n\ntokenizer = Wav2Vec2CTCTokenizer(\"/kaggle/working/vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n\nfeature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n\nprocessor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:29.381303Z","iopub.execute_input":"2022-12-18T20:00:29.382011Z","iopub.status.idle":"2022-12-18T20:00:29.763389Z","shell.execute_reply.started":"2022-12-18T20:00:29.381978Z","shell.execute_reply":"2022-12-18T20:00:29.762387Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import IPython.display as ipd\nimport numpy as np\nimport random","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:29.765196Z","iopub.execute_input":"2022-12-18T20:00:29.765551Z","iopub.status.idle":"2022-12-18T20:00:29.770941Z","shell.execute_reply.started":"2022-12-18T20:00:29.765517Z","shell.execute_reply":"2022-12-18T20:00:29.769385Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# it = iter(common_voice_train)\n# next(it)\n# sample = next(it)","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:29.772689Z","iopub.execute_input":"2022-12-18T20:00:29.773060Z","iopub.status.idle":"2022-12-18T20:00:29.780637Z","shell.execute_reply.started":"2022-12-18T20:00:29.773027Z","shell.execute_reply":"2022-12-18T20:00:29.779742Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# ipd.Audio(data=sample[\"audio\"][\"array\"], autoplay=True, rate=16000)","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:29.785093Z","iopub.execute_input":"2022-12-18T20:00:29.785339Z","iopub.status.idle":"2022-12-18T20:00:29.792083Z","shell.execute_reply.started":"2022-12-18T20:00:29.785317Z","shell.execute_reply":"2022-12-18T20:00:29.791286Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def prepare_dataset(batch):\n    audio = batch[\"audio\"]\n\n    # batched output is \"un-batched\"\n    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n    \n    with processor.as_target_processor():\n#         batch[\"labels\"] = processor(batch[\"sentence\"]).input_ids\n        batch[\"labels\"] = processor(batch[\"transcription\"]).input_ids\n        \n    return batch","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:29.794585Z","iopub.execute_input":"2022-12-18T20:00:29.794888Z","iopub.status.idle":"2022-12-18T20:00:29.803597Z","shell.execute_reply.started":"2022-12-18T20:00:29.794850Z","shell.execute_reply":"2022-12-18T20:00:29.802583Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"common_voice_train = common_voice_train.map(prepare_dataset)\ncommon_voice_test = common_voice_test.map(prepare_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:29.806651Z","iopub.execute_input":"2022-12-18T20:00:29.806926Z","iopub.status.idle":"2022-12-18T20:00:29.817303Z","shell.execute_reply.started":"2022-12-18T20:00:29.806899Z","shell.execute_reply":"2022-12-18T20:00:29.816448Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# it = iter(common_voice_train)\n# next(it)\n# sample = next(it)","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:29.819477Z","iopub.execute_input":"2022-12-18T20:00:29.819758Z","iopub.status.idle":"2022-12-18T20:00:29.827953Z","shell.execute_reply.started":"2022-12-18T20:00:29.819734Z","shell.execute_reply":"2022-12-18T20:00:29.827072Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# import torchaudio\n\n# def change_speed(sample):\n#     speed_factor= 1.0\n#     sample_rate = 16000\n    \n#     sox_effects = [\n#             [\"speed\", str(speed_factor)],\n#             [\"rate\", str(sample_rate)],\n#             ['norm'],\n#         ]\n#     transformed_audio, _ = torchaudio.sox_effects.apply_effects_tensor(\n#             torch.unsqueeze(torch.from_numpy(sample['input_values']), 0), sample_rate, sox_effects)\n    \n#     noise_effect = [['synth', '0', 'whitenoise']]\n\n#     noise, _ = torchaudio.sox_effects.apply_effects_tensor(\n#                 transformed_audio, sample_rate, noise_effect)\n\n        \n#     return transformed_audio+noise/50","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:29.829091Z","iopub.execute_input":"2022-12-18T20:00:29.829377Z","iopub.status.idle":"2022-12-18T20:00:29.838009Z","shell.execute_reply.started":"2022-12-18T20:00:29.829354Z","shell.execute_reply":"2022-12-18T20:00:29.837034Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# ipd.Audio(data=change_speed(sample), autoplay=True, rate=16000)","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:29.839182Z","iopub.execute_input":"2022-12-18T20:00:29.839583Z","iopub.status.idle":"2022-12-18T20:00:29.847805Z","shell.execute_reply.started":"2022-12-18T20:00:29.839559Z","shell.execute_reply":"2022-12-18T20:00:29.846881Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchaudio\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Union\n\n@dataclass\nclass DataCollatorCTCWithPadding:\n    \"\"\"\n    Data collator that will dynamically pad the inputs received.\n    Args:\n        processor (:class:`~transformers.Wav2Vec2Processor`)\n            The processor used for proccessing the data.\n        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n            among:\n            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n              sequence if provided).\n            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n              maximum acceptable input length for the model if that argument is not provided.\n            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n              different lengths).\n        max_length (:obj:`int`, `optional`):\n            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n        max_length_labels (:obj:`int`, `optional`):\n            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n        pad_to_multiple_of (:obj:`int`, `optional`):\n            If set will pad the sequence to a multiple of the provided value.\n            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n            7.5 (Volta).\n    \"\"\"\n\n    processor: Wav2Vec2Processor\n    padding: Union[bool, str] = True\n    max_length: Optional[int] = None\n    max_length_labels: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    pad_to_multiple_of_labels: Optional[int] = None\n          \n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # split inputs and labels since they have to be of different lenghts and need\n        # different padding methods\n        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n\n        batch = self.processor.pad(\n            input_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=\"pt\",\n        )\n        \n        with self.processor.as_target_processor():\n            labels_batch = self.processor.pad(\n                label_features,\n                padding=self.padding,\n                max_length=self.max_length_labels,\n                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n                return_tensors=\"pt\",\n            )\n\n        # replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n        batch[\"labels\"] = labels\n\n        return batch","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:29.849297Z","iopub.execute_input":"2022-12-18T20:00:29.849690Z","iopub.status.idle":"2022-12-18T20:00:30.015503Z","shell.execute_reply.started":"2022-12-18T20:00:29.849657Z","shell.execute_reply":"2022-12-18T20:00:30.011999Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def speed_transform(batch, p=0.5):\n    \n    sample_rate=16000\n    speed_factor = random.choice([0.9, 1.0, 1.1])\n\n#     if speed_factor == 1.0: # no change\n#         return batch\n    \n    if random.random()<p:\n        # change speed and resample to original rate:\n        sox_effects = [\n            [\"speed\", str(speed_factor)],\n            [\"rate\", str(sample_rate)],\n            ['norm'],\n#             ['echo',  '0.8', '0.88', '100', '0.4']\n#             ['reverb', \"-w\"]\n        ]\n        transformed_audio, _ = torchaudio.sox_effects.apply_effects_tensor(\n            torch.unsqueeze(torch.from_numpy(batch[\"input_values\"]), 0), sample_rate, sox_effects)\n        \n        noise_effect = [['synth', '0', 'whitenoise']]\n\n        noise, _ = torchaudio.sox_effects.apply_effects_tensor(\n                    transformed_audio, sample_rate, noise_effect)\n\n        batch['input_values'] = torch.squeeze(transformed_audio+noise/50).numpy()\n\n    return batch","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:30.836515Z","iopub.execute_input":"2022-12-18T20:00:30.836879Z","iopub.status.idle":"2022-12-18T20:00:30.844127Z","shell.execute_reply.started":"2022-12-18T20:00:30.836847Z","shell.execute_reply":"2022-12-18T20:00:30.843152Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"common_voice_train = common_voice_train.map(speed_transform)","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:31.250860Z","iopub.execute_input":"2022-12-18T20:00:31.251218Z","iopub.status.idle":"2022-12-18T20:00:31.256878Z","shell.execute_reply.started":"2022-12-18T20:00:31.251188Z","shell.execute_reply":"2022-12-18T20:00:31.255619Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:34.224581Z","iopub.execute_input":"2022-12-18T20:00:34.225071Z","iopub.status.idle":"2022-12-18T20:00:34.230325Z","shell.execute_reply.started":"2022-12-18T20:00:34.225030Z","shell.execute_reply":"2022-12-18T20:00:34.229185Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install jiwer","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:34.591126Z","iopub.execute_input":"2022-12-18T20:00:34.591574Z","iopub.status.idle":"2022-12-18T20:00:47.239764Z","shell.execute_reply.started":"2022-12-18T20:00:34.591531Z","shell.execute_reply":"2022-12-18T20:00:47.238487Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"wer_metric = load_metric(\"wer\")","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:47.242415Z","iopub.execute_input":"2022-12-18T20:00:47.242829Z","iopub.status.idle":"2022-12-18T20:00:47.810918Z","shell.execute_reply.started":"2022-12-18T20:00:47.242791Z","shell.execute_reply":"2022-12-18T20:00:47.809886Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.90k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"853d1171ed79464f9f58494f3f2ff1a8"}},"metadata":{}}]},{"cell_type":"code","source":"def compute_metrics(pred):\n    pred_logits = pred.predictions\n    pred_ids = np.argmax(pred_logits, axis=-1)\n\n    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n\n    pred_str = processor.batch_decode(pred_ids)\n    # we do not want to group tokens when computing the metrics\n    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n\n    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n\n    return {\"wer\": wer}","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:47.812303Z","iopub.execute_input":"2022-12-18T20:00:47.813640Z","iopub.status.idle":"2022-12-18T20:00:47.821083Z","shell.execute_reply.started":"2022-12-18T20:00:47.813603Z","shell.execute_reply":"2022-12-18T20:00:47.819532Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"from transformers import Wav2Vec2ForCTC\n\nmodel = Wav2Vec2ForCTC.from_pretrained(\n#     \"facebook/wav2vec2-base-100k-voxpopuli\", \n#     '/kaggle/working/model_folder/model',\n    '/kaggle/working/model_folder', \n    attention_dropout=0.1,\n    hidden_dropout=0.1,\n    feat_proj_dropout=0.0,\n    mask_time_prob=0.05,\n    layerdrop=0.1,\n    ctc_loss_reduction=\"mean\", \n    pad_token_id=processor.tokenizer.pad_token_id,\n    vocab_size=len(processor.tokenizer)\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:47.824195Z","iopub.execute_input":"2022-12-18T20:00:47.824602Z","iopub.status.idle":"2022-12-18T20:00:50.332680Z","shell.execute_reply.started":"2022-12-18T20:00:47.824565Z","shell.execute_reply":"2022-12-18T20:00:50.331440Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"model.freeze_feature_extractor()","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:50.334510Z","iopub.execute_input":"2022-12-18T20:00:50.335305Z","iopub.status.idle":"2022-12-18T20:00:50.346094Z","shell.execute_reply.started":"2022-12-18T20:00:50.335259Z","shell.execute_reply":"2022-12-18T20:00:50.344870Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1619: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"model.gradient_checkpointing_enable()","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:50.348111Z","iopub.execute_input":"2022-12-18T20:00:50.348606Z","iopub.status.idle":"2022-12-18T20:00:50.360562Z","shell.execute_reply.started":"2022-12-18T20:00:50.348542Z","shell.execute_reply":"2022-12-18T20:00:50.359482Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"%%capture\n\n!pip install neptune-client","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:00:50.361998Z","iopub.execute_input":"2022-12-18T20:00:50.362545Z","iopub.status.idle":"2022-12-18T20:01:02.258155Z","shell.execute_reply.started":"2022-12-18T20:00:50.362507Z","shell.execute_reply":"2022-12-18T20:01:02.256630Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"import neptune.new as neptune","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:01:02.259918Z","iopub.execute_input":"2022-12-18T20:01:02.260349Z","iopub.status.idle":"2022-12-18T20:01:02.522456Z","shell.execute_reply.started":"2022-12-18T20:01:02.260302Z","shell.execute_reply":"2022-12-18T20:01:02.521331Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"import os","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:01:02.525024Z","iopub.execute_input":"2022-12-18T20:01:02.525972Z","iopub.status.idle":"2022-12-18T20:01:02.531374Z","shell.execute_reply.started":"2022-12-18T20:01:02.525932Z","shell.execute_reply":"2022-12-18T20:01:02.530278Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"os.environ['NEPTUNE_API_TOKEN'] = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI3NTBlYjM5ZC1lZGYwLTQ3NGMtODg3Ni1mY2NmOWY4OGQzN2QifQ==\"\nos.environ['NEPTUNE_PROJECT'] = 'aibabynin/hw-audio2'","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:01:02.535651Z","iopub.execute_input":"2022-12-18T20:01:02.535997Z","iopub.status.idle":"2022-12-18T20:01:02.544468Z","shell.execute_reply.started":"2022-12-18T20:01:02.535963Z","shell.execute_reply":"2022-12-18T20:01:02.543341Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"optimizer =  torch.optim.AdamW(model.parameters(), lr=1e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:01:02.545766Z","iopub.execute_input":"2022-12-18T20:01:02.546367Z","iopub.status.idle":"2022-12-18T20:01:02.556561Z","shell.execute_reply.started":"2022-12-18T20:01:02.546331Z","shell.execute_reply":"2022-12-18T20:01:02.555255Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n  # output_dir=\"/content/gdrive/MyDrive/wav2vec2-large-xlsr-turkish-demo\",\n  output_dir=\"./wav2vec2-basee-ru-demo\",\n  group_by_length=False,\n  per_device_train_batch_size=25,\n  gradient_accumulation_steps=16,\n  evaluation_strategy=\"steps\",\n  num_train_epochs=300,\n  fp16=True,\n  save_steps=50,\n  eval_steps=50,\n  logging_steps=10,\n#   learning_rate=3e-4,\n#   warmup_steps=50,\n  save_total_limit=1,\n    max_steps=300,\n    report_to='neptune'\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:01:02.558062Z","iopub.execute_input":"2022-12-18T20:01:02.558454Z","iopub.status.idle":"2022-12-18T20:01:07.768422Z","shell.execute_reply.started":"2022-12-18T20:01:02.558419Z","shell.execute_reply":"2022-12-18T20:01:07.767684Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    data_collator=data_collator,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=common_voice_train.with_format(\"torch\"),\n    eval_dataset=common_voice_test.with_format(\"torch\"),\n#     train_dataset=common_voice_train,\n#     eval_dataset=common_voice_test,\n    tokenizer=processor.feature_extractor,\n    optimizers=(optimizer, scheduler)\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:01:07.769693Z","iopub.execute_input":"2022-12-18T20:01:07.770357Z","iopub.status.idle":"2022-12-18T20:01:13.566877Z","shell.execute_reply.started":"2022-12-18T20:01:07.770321Z","shell.execute_reply":"2022-12-18T20:01:13.565933Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\nUsing cuda_amp half precision backend\n","output_type":"stream"}]},{"cell_type":"code","source":"#with augmentations\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-12-18T20:01:13.568418Z","iopub.execute_input":"2022-12-18T20:01:13.569185Z","iopub.status.idle":"2022-12-18T22:46:14.003496Z","shell.execute_reply.started":"2022-12-18T20:01:13.569142Z","shell.execute_reply":"2022-12-18T22:46:14.002464Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"***** Running training *****\n  Num examples = 240000\n  Num Epochs = 9223372036854775807\n  Instantaneous batch size per device = 25\n  Total train batch size (w. parallel, distributed & accumulation) = 800\n  Gradient Accumulation steps = 16\n  Total optimization steps = 300\n/opt/conda/lib/python3.7/site-packages/transformers/integrations.py:928: NeptuneDeprecationWarning: `init` is deprecated, use `init_run` instead. We'll end support of it in `neptune-client==1.0.0`.\n  run=os.getenv(\"NEPTUNE_RUN_ID\", None),\n/opt/conda/lib/python3.7/site-packages/neptune/new/internal/init/__init__.py:43: NeptuneDeprecationWarning: Parameter `run` is deprecated, use `with_id` instead. We'll end support of it in `neptune-client==1.0.0`.\n  return init_run(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"https://app.neptune.ai/aibabynin/hw-audio2/e/HWAUD-14\nRemember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/neptune/new/attributes/attribute.py:64: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` instead.\n  return self.assign(value, wait)\nThe following columns in the training set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: audio, transcription. If audio, transcription are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [300/300 2:44:12, Epoch 13/9223372036854775807]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Wer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.510800</td>\n      <td>0.631781</td>\n      <td>0.675057</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.492600</td>\n      <td>0.643725</td>\n      <td>0.668376</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.445400</td>\n      <td>0.647678</td>\n      <td>0.674699</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.451100</td>\n      <td>0.639618</td>\n      <td>0.661935</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.411000</td>\n      <td>0.647588</td>\n      <td>0.673744</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.477000</td>\n      <td>0.631322</td>\n      <td>0.669689</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples: Unknown\n  Batch size = 16\nThe following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: audio, transcription. If audio, transcription are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\nSaving model checkpoint to ./wav2vec2-basee-ru-demo/checkpoint-50\nConfiguration saved in ./wav2vec2-basee-ru-demo/checkpoint-50/config.json\nModel weights saved in ./wav2vec2-basee-ru-demo/checkpoint-50/pytorch_model.bin\nFeature extractor saved in ./wav2vec2-basee-ru-demo/checkpoint-50/preprocessor_config.json\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n***** Running Evaluation *****\n  Num examples: Unknown\n  Batch size = 16\nThe following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: audio, transcription. If audio, transcription are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\nSaving model checkpoint to ./wav2vec2-basee-ru-demo/checkpoint-100\nConfiguration saved in ./wav2vec2-basee-ru-demo/checkpoint-100/config.json\nModel weights saved in ./wav2vec2-basee-ru-demo/checkpoint-100/pytorch_model.bin\nFeature extractor saved in ./wav2vec2-basee-ru-demo/checkpoint-100/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-basee-ru-demo/checkpoint-50] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n***** Running Evaluation *****\n  Num examples: Unknown\n  Batch size = 16\nThe following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: audio, transcription. If audio, transcription are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\nSaving model checkpoint to ./wav2vec2-basee-ru-demo/checkpoint-150\nConfiguration saved in ./wav2vec2-basee-ru-demo/checkpoint-150/config.json\nModel weights saved in ./wav2vec2-basee-ru-demo/checkpoint-150/pytorch_model.bin\nFeature extractor saved in ./wav2vec2-basee-ru-demo/checkpoint-150/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-basee-ru-demo/checkpoint-100] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n***** Running Evaluation *****\n  Num examples: Unknown\n  Batch size = 16\nThe following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: audio, transcription. If audio, transcription are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\nSaving model checkpoint to ./wav2vec2-basee-ru-demo/checkpoint-200\nConfiguration saved in ./wav2vec2-basee-ru-demo/checkpoint-200/config.json\nModel weights saved in ./wav2vec2-basee-ru-demo/checkpoint-200/pytorch_model.bin\nFeature extractor saved in ./wav2vec2-basee-ru-demo/checkpoint-200/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-basee-ru-demo/checkpoint-150] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n***** Running Evaluation *****\n  Num examples: Unknown\n  Batch size = 16\nThe following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: audio, transcription. If audio, transcription are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\nSaving model checkpoint to ./wav2vec2-basee-ru-demo/checkpoint-250\nConfiguration saved in ./wav2vec2-basee-ru-demo/checkpoint-250/config.json\nModel weights saved in ./wav2vec2-basee-ru-demo/checkpoint-250/pytorch_model.bin\nFeature extractor saved in ./wav2vec2-basee-ru-demo/checkpoint-250/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-basee-ru-demo/checkpoint-200] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n***** Running Evaluation *****\n  Num examples: Unknown\n  Batch size = 16\nThe following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: audio, transcription. If audio, transcription are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\nSaving model checkpoint to ./wav2vec2-basee-ru-demo/checkpoint-300\nConfiguration saved in ./wav2vec2-basee-ru-demo/checkpoint-300/config.json\nModel weights saved in ./wav2vec2-basee-ru-demo/checkpoint-300/pytorch_model.bin\nFeature extractor saved in ./wav2vec2-basee-ru-demo/checkpoint-300/preprocessor_config.json\nDeleting older checkpoint [wav2vec2-basee-ru-demo/checkpoint-250] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=300, training_loss=0.4725435733795166, metrics={'train_runtime': 9900.4105, 'train_samples_per_second': 24.241, 'train_steps_per_second': 0.03, 'total_flos': 9.124907585058109e+18, 'train_loss': 0.4725435733795166, 'epoch': 13.0})"},"metadata":{}}]},{"cell_type":"code","source":"!tar -zcvf model_1712_1446_steps.tar.gz wav2vec2-basee-ru-demo","metadata":{"execution":{"iopub.status.busy":"2022-12-18T22:47:56.421678Z","iopub.execute_input":"2022-12-18T22:47:56.422286Z","iopub.status.idle":"2022-12-18T22:48:55.357364Z","shell.execute_reply.started":"2022-12-18T22:47:56.422250Z","shell.execute_reply":"2022-12-18T22:48:55.356257Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"wav2vec2-basee-ru-demo/\nwav2vec2-basee-ru-demo/checkpoint-300/\nwav2vec2-basee-ru-demo/checkpoint-300/scaler.pt\nwav2vec2-basee-ru-demo/checkpoint-300/scheduler.pt\nwav2vec2-basee-ru-demo/checkpoint-300/training_args.bin\nwav2vec2-basee-ru-demo/checkpoint-300/trainer_state.json\nwav2vec2-basee-ru-demo/checkpoint-300/pytorch_model.bin\nwav2vec2-basee-ru-demo/checkpoint-300/config.json\nwav2vec2-basee-ru-demo/checkpoint-300/rng_state.pth\nwav2vec2-basee-ru-demo/checkpoint-300/optimizer.pt\nwav2vec2-basee-ru-demo/checkpoint-300/preprocessor_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink","metadata":{"execution":{"iopub.status.busy":"2022-12-18T22:50:22.032306Z","iopub.execute_input":"2022-12-18T22:50:22.033274Z","iopub.status.idle":"2022-12-18T22:50:22.038337Z","shell.execute_reply.started":"2022-12-18T22:50:22.033236Z","shell.execute_reply":"2022-12-18T22:50:22.037285Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"FileLink(r'model_1712_1446_steps.tar.gz')","metadata":{"execution":{"iopub.status.busy":"2022-12-18T22:50:28.015484Z","iopub.execute_input":"2022-12-18T22:50:28.015858Z","iopub.status.idle":"2022-12-18T22:50:28.023416Z","shell.execute_reply.started":"2022-12-18T22:50:28.015827Z","shell.execute_reply":"2022-12-18T22:50:28.022191Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/model_1712_1446_steps.tar.gz","text/html":"<a href='model_1712_1446_steps.tar.gz' target='_blank'>model_1712_1446_steps.tar.gz</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"import gc","metadata":{"execution":{"iopub.status.busy":"2022-12-18T22:50:36.930850Z","iopub.execute_input":"2022-12-18T22:50:36.931242Z","iopub.status.idle":"2022-12-18T22:50:36.935963Z","shell.execute_reply.started":"2022-12-18T22:50:36.931210Z","shell.execute_reply":"2022-12-18T22:50:36.934798Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-12-18T22:50:37.870552Z","iopub.execute_input":"2022-12-18T22:50:37.870947Z","iopub.status.idle":"2022-12-18T22:50:38.187633Z","shell.execute_reply.started":"2022-12-18T22:50:37.870918Z","shell.execute_reply":"2022-12-18T22:50:38.186435Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"97"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}